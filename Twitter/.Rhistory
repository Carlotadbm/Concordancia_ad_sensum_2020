tibble(folder = dir(infolder, full.names = TRUE)) %>%
mutate(file = map(folder, dir_ls)) %>%
unnest(file) %>%
mutate(text = map(file, read_lines)) %>%
unnest(text) %>%
mutate(folder = str_replace(folder, "/Users/carlotadebenitomoreno/Documents/Corpus PostScriptum/Descargado//ES", ""),
folder = str_replace(folder, "_ORIG_TXT", ""),
file = str_replace(file, "/Users/carlotadebenitomoreno/Documents/Corpus PostScriptum/Descargado/ES1[5678]00_ORIG_TXT/", ""))
tibble(folder = dir(infolder, full.names = TRUE)) %>%
mutate(file = map(folder, dir_ls)) %>%
unnest(file) %>%
mutate(text = map(file, read_lines)) %>%
unnest(text) %>%
mutate(folder = str_replace(folder, "/Users/carlotadebenitomoreno/Documents/Corpus PostScriptum/Descargado//ES", ""),
folder = str_replace(folder, "_ORIG_TXT", ""),
file = str_replace(file, "/Users/carlotadebenitomoreno/Documents/Corpus PostScriptum/Descargado/ES1[5678]00_ORIG_TXT/", ""),
file = str_replace(file, ".txt", ""))
tibble(century = dir(infolder, full.names = TRUE)) %>%
mutate(textID = map(century, dir_ls)) %>%
unnest(textID) %>%
mutate(text = map(textID, read_lines)) %>%
unnest(text) %>%
mutate(century = str_replace(century, "/Users/carlotadebenitomoreno/Documents/Corpus PostScriptum/Descargado//ES", ""),
century = str_replace(century, "_ORIG_TXT", ""),
textID = str_replace(textID, "/Users/carlotadebenitomoreno/Documents/Corpus PostScriptum/Descargado/ES1[5678]00_ORIG_TXT/", ""),
textID = str_replace(textID, ".txt", ""))
install.packages("swirl")
library(swirl)
install_course_github(
"Carlotadbm",
"Tools_for_text_analysis")
swirl()
swirl()
swirl()
library(tidyverse)
library(tidytext)
?unnest_tokens
library(swirl)
swirl()
quijote
verne
str_detect(quijote$word, "$a")
str_detect(quijote$word, "^a")
str_detect(quijote$word, "^a") %>% sum()
str_detect(quijote$word, "^a") %>% mean()
quijote %>% mutate(starts_a = str_detect(word, "^a"))
quijote %>% filter(str_detect(word, "^a"))
quijote %>% filter(!str_detect(word, "[aeiouáéíóúü]"))
str_count(quijote$word, "a")
str_count(quijote$word, "a") %>% mean()
quijote %>% mutate(number_a = str_count(word, "a"))
str_count("abababa", "aba")
str_view_all("abababa", "aba")
skip()
swirl()
library(swirlify)
demo_lesson()
demo_lesson(15)
demo_lesson(18)
demo_lesson(21)
skip()
demo_lesson(22)
skip()
demo_lesson(22)
verne %>% mutate(fem_noun = str_extract(sentences, "\\bla\\b\\s\\w+\\b"))
demo_lesson(22)
verne %>% mutate(fem_noun = str_extract(sentences, "\\bla\\b\\s\\w+\\b"))
test_lesson()
demo_lesson(22)
skip()
skip()
skip()
skip()
skip()
skip()
skip()
library(swirlify)
test_lesson()
swirl()
demo_lesson()
skip()
library(swirl)
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
install.packages("swirl")
library(swirl)
install_course_github(
"Carlotadbm",
"Tools_for_text_analysis")
swirl()
library(tidyverse)
#library(XML)
library(xml2)
file_url2 <- "https://raw.githubusercontent.com/noe-eva/NOAH-Corpus/master/blick.xml"
data2 <- read_xml(file_url2)
xml_structure(data2)
xml_find_all(data2, "//s") #finds all s objects
sentences <- xml_find_all(data2, "//s") #finds all s objects
xml_text(sentences) #extracts content from arguments It reads the sentences without spaces :/
#xml_double
#xml_integer #also exist
as_list(sentences) #extracts content as list
words <- xml_find_all(data2, "//w") #finds all w objects
xml_text(words)
word1 <- xml_find_first(data2, "//w") #finds all w objects
xml_text(word1)
xml_attrs(word1) # Find all attributes with xml_attrs()
xml_attrs(words) # Find all attributes with xml_attrs()
xml_attr(word1, attr = "pos") # Find pos attribute with xml_attr()
xml_attr(words, attr = "pos") # Find pos attribute for all w nodes
words <- xml_find_all(data2, "//w") #finds all w objects
words_c <- xml_text(words)
word_id <- xml_attr(words, attr = "n")
pos_attr <- xml_attr(words, attr = "pos")
verified_attr <- xml_attr(words, attr = "verified")
words_df <- tibble(words_c, word_id, pos_attr, verified_attr)
articles <- xml_find_all(data2, "//article")
#articles_c <- xml_text(articles)
article_id <- xml_attr(articles, attr = "n")
dialect_attr <- xml_attr(articles, attr = "dialect")
title_attr <- xml_attr(articles, attr = "title")
articles_df <- tibble(article_id, dialect_attr, title_attr)
sentences <- xml_find_all(data2, "//s")
sentences_c <- xml_text(sentences) #El problema es que no hay un contenido de la frase, sino que pega las diferentes palabras
sentences_id <- xml_attr(sentences, attr = "n")
sentences_df <- tibble(sentences_id, sentences_c)
words_df
articles_df
sentences_df
xml_structure(data2)
xml_structure(data2) %>% head()
xml_structure(data2) %>% head(n=1)
library(XML)
xmlTreeParse(data2)
xmlTreeParse(data2) %>%
?xmlTreeParse
?xmlTreeParse
xmlTreeParse(data2) %>% head(n=1)
words_df %>%
count(pos, sort = T) %>%
mutate(pos = reorder(pos, n)) %>%
ggplot(aes(pos, n)) +
geom_col() +
coord_flip()
words_df
words_df %>%
count(pos_attr, sort = T) %>%
mutate(pos_attr = reorder(pos_attr, n)) %>%
ggplot(aes(pos_attr, n)) +
geom_col() +
coord_flip()
words_df
words_df <- tibble(words_c, word_id, pos_attr, verified_attr) %>%
separate(word_id, into = c("article_id", "sentence_id", "word_id"))
#words_df <-
tibble(words_c, word_id, pos_attr, verified_attr) %>%
separate(word_id, into = c("article_id", "sentence_id", "word_id"))
#words_df <-
tibble(words_c, word_id, pos_attr, verified_attr) %>%
separate(word_id, into = c("article_id", "sentence_id", "word_id")) %>%
mutate(sentence_id = str_c(article_id, "-", sentence_id))
#words_df <-
tibble(words_c, word_id, pos_attr, verified_attr) %>%
mutate(article_id = word_id) %>%
separate(article_id, into = c("article_id", "sentence_id", "word")) %>%
mutate(sentence_id = str_c(article_id, "-", sentence_id)) %>%
select(-word)
#words_df <-
tibble(words_c, word_id, pos_attr, verified_attr) %>%
separate(word_id, into = c("article_id", "sentence_id", "word_id"))
words_df <-
tibble(words_c, word_id, pos_attr, verified_attr) %>%
separate(word_id, into = c("article_id", "sentence_id", "word_id"))
?lead
words_df %>%
distinct(pos_attr)
words_df %>%
count(pos_attr, sort = T)
unique(words_df$pos_attr)
words_df %>%
group_by(article_id, sentence_id) %>%
mutate(word_next = lag(words_c),
word_after = lead(words_c))
words_df %>%
group_by(article_id, sentence_id) %>%
mutate(word_next = lag(words_c),
word_after = lead(words_c),
pos_next = lag(pos_attr),
pos_after = lead(pos_attr))
words_df %>%
group_by(article_id, sentence_id) %>%
mutate(word_before = lag(words_c),
word_after = lead(words_c),
pos_before = lag(pos_attr),
pos_after = lead(pos_attr)) %>%
filter(pos_before == "PPOSAT" & pos_attr == "NN")
words_df %>%
group_by(article_id, sentence_id) %>%
mutate(word_before = lag(words_c),
word_after = lead(words_c),
pos_before = lag(pos_attr),
pos_after = lead(pos_attr)) %>%
filter(pos_before == "PPOSAT" & pos_attr == "NN") %>%
count(word_before, words_c, sort = T)
words_df %>%
group_by(article_id, sentence_id) %>%
mutate(word_before = lag(words_c),
word_after = lead(words_c),
pos_before = lag(pos_attr),
pos_after = lead(pos_attr)) #%>%
words_df %>%
group_by(article_id, sentence_id) %>%
mutate(word_before = lag(words_c),
word_after = lead(words_c),
pos_before = lag(pos_attr),
pos_after = lead(pos_attr)) %>%
tail()
add_2 <- function(x) {
x + 2
}
add_2(5)
?join
library(tidyverse)
?join
?anti_join
library(swirl)
swirl()
subjects2
coords2
subjects2 %>%
count(COSERID, number)
subjects2 %>%
count(COSERID, number) %>%
spread(number, n, fill = 0)
subjects2 %>%
count(COSERID, number) %>%
spread(number, n, fill = 0) %>%
group_by(COSERID)
swirl()
subjects2_map <- subjects2 %>%
count(COSERID, number) %>%
spread(number, n, fill = 0) %>%
group_by(COSERID) %>%
mutate(total = sum(sg, pl)) %>%
#mutate(total = sg + pl) %>%
left_join(coords2)
spain <- map_data("world", c("Spain", "Canary"))
ggplot(spain, aes(x = long, y = lat)) +
geom_map(map = spain, aes(map_id = region), fill = "white", colour = "black") +
geom_scatterpie(data = subjects2_map, aes(x = longitude, y = latitude, r = sqrt(total)/15), cols = c("sg", "pl"), alpha = 0.5) +
scale_fill_manual(breaks = c("sg", "pl"), labels = c("3sg", "3pl"), values = c("sg" = "white", "pl" = "red")) +
coord_map() +
labs(title = "Verbal agreement with singular subjects") +
theme_bw() +
theme(panel.grid = element_blank(), panel.border = element_blank(),
axis.title = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(),
legend.title = element_blank(), legend.position = c(1, 0), legend.justification = c(1, 0))
library(swirlify)
library(swirl)
install.packages("swirl")
install.packages("swirl")
library(swirl)
install_course_github(
"Carlotadbm",
"Tools_for_text_analysis")
swirl()
install.packages("swirl")
library(swirl)
install_course_github(
"Carlotadbm",
"Tools_for_text_analysis")
swirl()
swirl()
?udpipe_download_model
udpipe_download_model("english-ewt")
udpipe_load_model("english-ewt-ud-2.5-191206.udpipe")
en_ewt <- udpipe_load_model("english-ewt-ud-2.5-191206.udpipe")
udpipe_download_model("english-gum")
en_ewt <- udpipe_load_model("english-gum-ud-2.5-191206.udpipe")
en_ewt <- udpipe_load_model("english-ewt-ud-2.5-191206.udpipe")
en_gum <- udpipe_load_model("english-gum-ud-2.5-191206.udpipe")
udpipe_download_model("english-lines")
en_lines <- udpipe_load_model("english-lines-ud-2.5-191206.udpipe")
udpipe_download_model("english-partut")
en_partut <- udpipe_load_model("english-partut-ud-2.5-191206.udpipe")
text
library(tidyverse)
#library(XML)
library(xml2)
file_url2 <- "https://raw.githubusercontent.com/noe-eva/NOAH-Corpus/master/blick.xml"
data2 <- read_xml(file_url2)
words <- xml_find_all(data2, "//w") #finds all w objects
words_c <- xml_text(words)
word_id <- xml_attr(words, attr = "n")
pos_attr <- xml_attr(words, attr = "pos")
verified_attr <- xml_attr(words, attr = "verified")
words_df <-
tibble(words_c, word_id, pos_attr, verified_attr) %>%
separate(word_id, into = c("article_id", "sentence_id", "word_id"))
#read file
url <- "https://raw.githubusercontent.com/noe-eva/NOAH-Corpus/master/blick.xml"
blick <- read_xml(url)
#read file
url <- "https://raw.githubusercontent.com/noe-eva/NOAH-Corpus/master/blick.xml"
blick <- read_xml(url)
url %>% read_xml()
#explore file
View(blick) #mejor
xml_structure(blick) #caótico, pero ok
blick %>% xml_structure()
#Convert into a tibble
#Articles is interesting, because it has the name of the article
articles <- xml_find_all(blick, "//article") %>% #finds all article objects
xml_attrs() %>%
enframe(name = NULL) %>%
unnest_wider(value) %>% #to transform into a tibble
rename(article_id = n)
#object s is irrelevant
xml_find_all(blick, "//s") %>%
xml_attrs %>%
enframe(name = NULL) %>%
unnest_wider(value)
#we get the attributes
words_attr <- xml_find_all(blick, "//w") %>%
xml_attrs %>%
enframe(name = NULL) %>%
unnest_wider(value) %>%
separate(n, into = c("article_id", "sentence_id", "word_id")) #we bind both words tables
#and the elements
words <- xml_find_all(blick, "//w") %>%
xml_text() %>%
enframe(name = NULL, value = "word") %>%
add_column(words_attr)
#now we join it with articles and we have all the information ready
corpus <- articles %>%
full_join(words)
##Probar Noah's
corpus
##Probar Noah's
corpus %>%
distinct(pos)
##Probar Noah's
corpus %>%
distinct(pos) %>%
print(n=70)
##Probar Noah's
corpus %>%
filter(str_detect(pos, "PTKNEG"))
##Probar Noah's
corpus %>%
filter(str_detect(pos, "PTKNEG")) %>%
distinct(word)
##Probar Noah's
corpus %>%
filter(str_detect(pos, "ART
")) %>%
distinct(word)
##Probar Noah's
corpus %>%
filter(str_detect(pos, "ART")) %>%
distinct(word)
##Probar Noah's
corpus %>%
filter(str_detect(pos, "ART")) %>%
distinct(word) %>%
print(n = Inf)
corpus %>%
filter(str_detect(word, "keis")) %>%
distinct(word) %>%
print(n = Inf)
corpus %>%
filter(str_detect(word, "keis"))
corpus %>%
filter(str_detect(word, "kei"))
##Probar Noah's
corpus %>%
filter(str_detect(pos, "PIAT")) %>%
distinct(word) %>%
print(n = Inf)
##Probar Noah's
corpus %>%
filter(str_detect(pos, "PIAT") & str_detect(word, "^[Kk]")) %>%
distinct(word) %>%
print(n = Inf)
corpus %>%
filter(str_detect(word, "kei"))
corpus %>%
group_by(article_id, sentence_id) %>%
mutate(sentence = str_c(word, collapse = " "))
corpus %>%
group_by(article_id, sentence_id) %>%
mutate(sentence = str_c(word, collapse = " ")) %>%
ungroup() %>%
filter(str_detect(pos, "PIAT") & str_detect(word, "^[Kk]"))
neg_art <- corpus %>%
group_by(article_id, sentence_id) %>%
mutate(sentence = str_c(word, collapse = " ")) %>%
ungroup() %>%
filter(str_detect(pos, "PIAT") & str_detect(word, "^[Kk]"))
View(neg_art)
setwd("~/switchdrive/UE Computational tools/Evaluacion_2/COSER TXT/Patrick")
setwd("~/switchdrive/UE Computational tools/Evaluacion_2/COSER TXT")
dir_ls()
library(tidyverse)
library(fs)
dir_ls()
setwd("~/switchdrive/UE Computational tools/Evaluacion_2/COSER TXT/Patrick")
dir_ls()
txt_files <- dir_ls()
txt_files
txt_files %>%
map_dfr(read_delim, delim = ";")
txt_files %>%
map_dfr(read_delim, delim = "\n")
read_delim("COSER-0222-01.TXT")
read_delim("COSER-0222-01.TXT", delim = ";")
read_delim("COSER-0222-01.TXT", delim = ";", col_names = FALSE)
read_delim("COSER-0222-01.TXT", delim = ";", col_names = "text")
txt_files %>%
map_dfr(read_delim, delim = ";", col_names = "text")
read_delim("COSER-0222-01.TXT", delim = ";", col_names = "text")
txt_files %>%
map_dfr(read_delim, delim = ";", col_names = "text")
txt_files %>%
map_dfr(read_delim, delim = "\n", col_names = "text")
read_delim("COSER-0222-01.TXT", delim = ";", col_names = "text")
txt_files %>%
map_dfr(read_delim, delim = "\n", col_names = "text")
txt_files %>%
map_dfr(read_delim, delim = ";", col_names = "text")
txt_files %>%
map_dfr(read_delim, delim = "\n", col_names = FALSE)
txt_files %>%
map_dfr(read_delim, delim = ";", col_names = FALSE)
txt_files %>%
map_dfr(read_delim, delim = "\n", col_names = "text") %>%
print(n=25)
txt_files %>%
map_dfr(read_delim, delim = "$", col_names = "text")
txt_files %>%
map_dfr(read_delim, delim = "\n", col_names = "text") %>%
print(n=25)
txt_files %>%
map_dfr(read_delim, delim = "\n", col_names = "text")
txt_files %>%
map_dfr(read_delim, delim = "$", col_names = "text")
read_lines("COSER-0222-01.TXT")
?read_lines
setwd("~/Desktop")
read_delim("OSTA-resultados-23-12-2020-20-02-38.tsv", delim = "\t")
mientre <- read_delim("OSTA-resultados-23-12-2020-20-02-38.tsv", delim = "\t")
mientre %>%
rename(Lema = `Término Buscado <Lema> [Etiqueta]`)
mientre %>%
rename(Lema = `Término Buscado <Lema> [Etiqueta]`) %>%
count(Lema)
setwd("~/Documents/Concordancia_ad_sensum_2020/Twitter")
read_delim("alguien_TW.csv", delim = "\t")
alguien <- read_delim("alguien_TW.csv", delim = "\t")
alguien
alguien$idtweets
alguien
alguien <- alguien %>%
select(-text)
alguien <- alguien %>%
select(-text) %>%
write_delim("alguien_TW.csv", delim = "\t")
alguien %>%
select(-text) %>%
write_delim("alguien_TW.csv", delim = "\t")
alguien <- read_delim("alguien_TW.csv", delim = "\t")
alguien %>%
select(-text) %>%
write_delim("alguien_TW.csv", delim = "\t")
alguien <- read_delim("elmundo_TW.csv", delim = "\t")
alguien %>%
select(-text) %>%
write_delim("elmundo_TW.csv", delim = "\t")
alguien <- read_delim("lafamilia_TW.csv", delim = "\t")
alguien %>%
select(-text) %>%
write_delim("lafamilia_TW.csv", delim = "\t")
alguien %>%
select(-text) %>%
write_delim("lagente_TW.csv", delim = "\t")
alguien <- read_delim("nadie_TW.csv", delim = "\t")
alguien %>%
select(-text) %>%
write_delim("nadie_TW.csv", delim = "\t")
alguien <- read_delim("lagente_TW.csv", delim = "\t")
alguien %>%
select(-text) %>%
write_delim("lagente_TW.csv", delim = "\t")
